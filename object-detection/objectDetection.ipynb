{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca8b413c-3993-4933-b899-b2edb0802f74",
   "metadata": {},
   "source": [
    "# Object Detection for Face Detection Project\n",
    "\n",
    "This notebook demonstrates the use of pre-trained object detection models, specifically focusing on detecting faces in images and live video feeds. The goal of this project is to implement and compare different object detection models, starting with YOLO (You Only Look Once), and assess their performance in terms of accuracy and speed.\n",
    "\n",
    "## Objectives:\n",
    "1. Research and select a suitable pre-trained object detection model (e.g., YOLOv5 or YOLOv8).\n",
    "2. Validate the model using a sample dataset to assess its performance.\n",
    "3. Implement the model to detect faces in static images and output annotated images with bounding boxes.\n",
    "4. Extend the functionality to handle live video feeds from a webcam, with real-time face detection and bounding box visualization.\n",
    "\n",
    "Throughout the notebook, we will evaluate key metrics such as accuracy (mAP) and inference speed (FPS), making adjustments as needed to optimize for both performance and accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d157ad50-97a1-4805-a86e-d9ca188dbc84",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e836d1da-44fa-4075-82d8-24bb1c7c9735",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "import cv2\n",
    "import zipfile\n",
    "import random\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a323e491-f4cd-4c09-a519-c25984a00d2e",
   "metadata": {},
   "source": [
    "### Create Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bce85f01-100d-4dbb-865f-f1c7f75961a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('Training'):\n",
    "    os.makedirs('Training')\n",
    "\n",
    "if not os.path.exists('Validation'):\n",
    "    os.makedirs('Validation')\n",
    "\n",
    "if not os.path.exists('Training/labels'):\n",
    "    os.makedirs('Training/labels')\n",
    "\n",
    "if not os.path.exists('Validation/labels'):\n",
    "    os.makedirs('Validation/labels')\n",
    "\n",
    "if not os.path.exists('Training/images'):\n",
    "    os.makedirs('Training/images')\n",
    "\n",
    "if not os.path.exists('Validation/images'):\n",
    "    os.makedirs('Validation/images')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47928c30-166b-483b-9d78-60ef255bf0de",
   "metadata": {},
   "source": [
    "### Unzip data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bb3150b9-fbdf-4ac7-8264-e4db5901d4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted all images from 'WIDER_train.zip' to 'Training\\images' successfully, with flattened directory structure.\n",
      "Extracted all images from 'WIDER_val.zip' to 'Validation\\images' successfully, with flattened directory structure.\n"
     ]
    }
   ],
   "source": [
    "def unzip_images_only(zip_file_path, dest_dir, images_location):\n",
    "    \"\"\"\n",
    "    Unzips only the 'images' folder from a given zip file to the specified destination directory.\n",
    "    It extracts all images from the 'WIDER_train/images' folder and places them directly into 'dest_dir/images'.\n",
    "\n",
    "    Parameters:\n",
    "    zip_file_path (str): Path to the zip file to be extracted.\n",
    "    dest_dir (str): Directory where the images should be extracted, into a subdirectory named 'images'.\n",
    "    images_location (str): The folder path inside the zip file where images are located.\n",
    "    \"\"\"\n",
    "    # Define the full path for the 'images' subdirectory\n",
    "    images_dest_dir = os.path.join(dest_dir, 'images')\n",
    "\n",
    "    # Ensure the 'images' subdirectory exists\n",
    "    if not os.path.exists(images_dest_dir):\n",
    "        os.makedirs(images_dest_dir)\n",
    "\n",
    "    # Open the zip file\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        for file_info in zip_ref.infolist():\n",
    "            # Extract only files from the specified 'images_location' folder\n",
    "            if file_info.filename.startswith(images_location) and not file_info.is_dir():\n",
    "                # Flatten the structure by moving all images directly into 'images_dest_dir'\n",
    "                # Extract the filename only (ignore subdirectories)\n",
    "                filename = os.path.basename(file_info.filename)\n",
    "                target_path = os.path.join(images_dest_dir, filename)\n",
    "                \n",
    "                # Extract the file to the target path\n",
    "                with zip_ref.open(file_info) as source, open(target_path, 'wb') as target:\n",
    "                    target.write(source.read())\n",
    "\n",
    "        print(f\"Extracted all images from '{zip_file_path}' to '{images_dest_dir}' successfully, with flattened directory structure.\")\n",
    "\n",
    "\n",
    "# Unzip train data\n",
    "zip_file_path_train = 'WIDER_train.zip'\n",
    "dest_dir_train = 'Training'\n",
    "images_location_train = 'WIDER_train/images/'\n",
    "\n",
    "if not os.listdir('Training/images'):\n",
    "    unzip_images_only(zip_file_path_train, dest_dir_train, images_location_train)\n",
    "\n",
    "# Unzip validation data\n",
    "zip_file_path_val = 'WIDER_val.zip'\n",
    "dest_dir_val = 'Validation'\n",
    "images_location_val = 'WIDER_val/images/'\n",
    "\n",
    "if not os.listdir('Validation/images'):\n",
    "    unzip_images_only(zip_file_path_val, dest_dir_val, images_location_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d45992d-cf0b-415c-9b69-d7a068cb2658",
   "metadata": {},
   "source": [
    "### Unzip GT data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "677ff123-1293-4ccd-8a44-f84f558c9b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip_file(zip_file_path, dest_dir=\".\"):\n",
    "    \"\"\"\n",
    "    Unzips a given zipped file to the specified destination directory.\n",
    "\n",
    "    Parameters:\n",
    "    zip_file_path (str): Path to the zip file to be extracted.\n",
    "    dest_dir (str): Directory where the zip file should be extracted.\n",
    "                    Default is the current working directory.\n",
    "    \"\"\"\n",
    "    # Check if the destination directory exists, if not, create it\n",
    "    if not os.path.exists(dest_dir):\n",
    "        os.makedirs(dest_dir)\n",
    "\n",
    "    # Unzipping the file\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(dest_dir)\n",
    "        print(f\"Extracted '{zip_file_path}' to '{os.path.abspath(dest_dir)}' successfully.\")\n",
    "\n",
    "# Example usage\n",
    "zip_file_path = 'wider_face_split.zip'\n",
    "\n",
    "if not os.path.exists(zip_file_path):\n",
    "    unzip_file(zip_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf92ff2-72fe-4d45-8e2e-bfb4e648899c",
   "metadata": {},
   "source": [
    "### Convert Annotations From the Wider Format to the YOLO Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bceca317-9003-44c6-ba0e-462d1f3ffc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to parse annotations and convert them to YOLO format\n",
    "def convert_to_yolo_format(annotations_file, images_root, output_labels_dir):\n",
    "    with open(annotations_file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        # Parse image path\n",
    "        image_path = lines[i].strip()\n",
    "\n",
    "        # Skip if this line does not represent an image path\n",
    "        if not image_path.endswith('.jpg'):\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # Extract only the filename (ignore any subdirectory in the annotation)\n",
    "        image_filename = os.path.basename(image_path)\n",
    "        image_full_path = os.path.join(images_root, image_filename)\n",
    "\n",
    "        # Skip if image does not exist\n",
    "        if not os.path.exists(image_full_path):\n",
    "            print(f\"Warning: Image {image_full_path} does not exist.\")\n",
    "            i += 1\n",
    "            # Skip to the next image by finding the next valid image path\n",
    "            while i < len(lines) and not lines[i].strip().endswith('.jpg'):\n",
    "                i += 1\n",
    "            continue\n",
    "\n",
    "        # Load image to get dimensions\n",
    "        image = cv2.imread(image_full_path)\n",
    "        if image is None:\n",
    "            print(f\"Error: Failed to load image {image_full_path}\")\n",
    "            i += 1\n",
    "            # Skip to the next image by finding the next valid image path\n",
    "            while i < len(lines) and not lines[i].strip().endswith('.jpg'):\n",
    "                i += 1\n",
    "            continue\n",
    "\n",
    "        image_height, image_width, _ = image.shape\n",
    "\n",
    "        # Parse number of bounding boxes\n",
    "        try:\n",
    "            num_boxes = int(lines[i + 1].strip())\n",
    "        except ValueError:\n",
    "            print(f\"Error: Expected an integer for number of bounding boxes but got: {lines[i + 1].strip()}\")\n",
    "            i += 1\n",
    "            # Skip to the next image by finding the next valid image path\n",
    "            while i < len(lines) and not lines[i].strip().endswith('.jpg'):\n",
    "                i += 1\n",
    "            continue\n",
    "\n",
    "        # Create the label file path in the output labels directory\n",
    "        label_path = os.path.join(output_labels_dir, image_filename.replace('.jpg', '.txt'))\n",
    "\n",
    "        with open(label_path, 'w') as label_file:\n",
    "            # Loop through bounding boxes\n",
    "            for j in range(num_boxes):\n",
    "                try:\n",
    "                    box_data = list(map(int, lines[i + 2 + j].split()[:4]))  # Extract x, y, width, height\n",
    "                    x, y, width, height = box_data\n",
    "\n",
    "                    # Convert to YOLO format\n",
    "                    x_center = (x + width / 2) / image_width\n",
    "                    y_center = (y + height / 2) / image_height\n",
    "                    width_normalized = width / image_width\n",
    "                    height_normalized = height / image_height\n",
    "\n",
    "                    # Write to label file in YOLO format\n",
    "                    label_file.write(f\"0 {x_center} {y_center} {width_normalized} {height_normalized}\\n\")\n",
    "                except ValueError:\n",
    "                    print(f\"Warning: Failed to parse bounding box for {image_filename}, skipping this bounding box.\")\n",
    "                    continue\n",
    "\n",
    "        # Move to the next image's annotations\n",
    "        i += 2 + num_boxes\n",
    "\n",
    "# Define paths\n",
    "\n",
    "# Training Data\n",
    "annotations_file_training = 'wider_face_split/wider_face_train_bbx_gt.txt'\n",
    "images_root_training = 'Training/images'\n",
    "output_labels_dir_training = 'Training/labels'\n",
    "\n",
    "if not os.path.exists(output_labels_dir_training):\n",
    "    os.makedirs(output_labels_dir_training)\n",
    "\n",
    "# Validation Data\n",
    "annotations_file_validation = 'wider_face_split/wider_face_val_bbx_gt.txt'\n",
    "images_root_validation = 'Validation/images'\n",
    "output_labels_dir_validation = 'Validation/labels'\n",
    "\n",
    "if not os.path.exists(output_labels_dir_validation):\n",
    "    os.makedirs(output_labels_dir_validation)\n",
    "\n",
    "# Convert the annotations to YOLO format\n",
    "if not os.listdir(output_labels_dir_training):\n",
    "    convert_to_yolo_format(annotations_file_training, images_root_training, output_labels_dir_training)\n",
    "\n",
    "if not os.listdir(output_labels_dir_validation):\n",
    "    convert_to_yolo_format(annotations_file_validation, images_root_validation, output_labels_dir_validation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0ab3b4-057b-402d-b599-2df011bf8b19",
   "metadata": {},
   "source": [
    "### Load Pre-Trained YOLO v8n model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a2dc5108-ba64-4d0c-a678-aed31090dcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('yolov8n.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39006d65-4623-4beb-889c-e99ebfab5c89",
   "metadata": {},
   "source": [
    "### Verify model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2a29fdf7-c5fc-457c-8f26-43cf666da175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly selected image: Training/images/35_Basketball_basketballgame_ball_35_158.jpg\n",
      "\n",
      "image 1/1 C:\\Users\\Alfred Aronsson\\SSY340-Project\\object-detection\\Training\\images\\35_Basketball_basketballgame_ball_35_158.jpg: 640x480 2 persons, 335.2ms\n",
      "Speed: 8.8ms preprocess, 335.2ms inference, 4.4ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    }
   ],
   "source": [
    "# Load your YOLO model\n",
    "model = YOLO('yolov8n.pt')\n",
    "\n",
    "# Define the path to the images directory\n",
    "images_dir = 'Training/images/'\n",
    "\n",
    "# List all image files in the directory\n",
    "image_files = [os.path.join(dp, f) for dp, dn, filenames in os.walk(images_dir) for f in filenames if f.endswith(('.jpg'))]\n",
    "\n",
    "# Ensure there are images available\n",
    "if len(image_files) == 0:\n",
    "    print(\"No images found in the directory.\")\n",
    "else:\n",
    "    # Randomly select an image from the list\n",
    "    random_image = random.choice(image_files)\n",
    "    print(f\"Randomly selected image: {random_image}\")\n",
    "\n",
    "    # Run inference on the selected image\n",
    "    results = model(random_image)\n",
    "\n",
    "    # Display the results\n",
    "    results[0].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d2ba0b-cd4a-48b6-8832-e45cfb96abea",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ade75e86-409f-4bc1-913d-37c7854a2dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.14 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.13  Python-3.11.9 torch-2.3.1 CPU (Intel Core(TM) i5-1035G7 1.20GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=data.yaml, epochs=50, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train3, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train3\n",
      "Downloading https://ultralytics.com/assets/Arial.ttf to 'C:\\Users\\Alfred Aronsson\\AppData\\Roaming\\Ultralytics\\Arial.ttf'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 755k/755k [00:00<00:00, 23.8MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    430867  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "Model summary: 249 layers, 2,690,403 parameters, 2,690,387 gradients, 6.9 GFLOPs\n",
      "\n",
      "Transferred 58/391 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train3', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\Alfred Aronsson\\SSY340-Project\\object-detection\\Training\\labels... 280 images, 12601 backgroun\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: C:\\Users\\Alfred Aronsson\\SSY340-Project\\object-detection\\Training\\labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Alfred Aronsson\\SSY340-Project\\object-detection\\Validation\\labels... 3226 images, 0 backgrounds,\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING  C:\\Users\\Alfred Aronsson\\SSY340-Project\\object-detection\\Validation\\images\\21_Festival_Festival_21_604.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING  C:\\Users\\Alfred Aronsson\\SSY340-Project\\object-detection\\Validation\\images\\39_Ice_Skating_iceskiing_39_583.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [1.0019531]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\Users\\Alfred Aronsson\\SSY340-Project\\object-detection\\Validation\\labels.cache\n",
      "Plotting labels to runs\\detect\\train3\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.9) with parameter groups 63 weight(decay=0.0), 70 weight(decay=0.0005), 69 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train3\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/50         0G      3.544      60.43       1.53          6        640:   1%|          | 10/805 [02:45<3:39:04, \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata.yaml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m640\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\dml\\Lib\\site-packages\\ultralytics\\engine\\model.py:802\u001b[0m, in \u001b[0;36mModel.train\u001b[1;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m    801\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mhub_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[1;32m--> 802\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m}:\n",
      "File \u001b[1;32m~\\.conda\\envs\\dml\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:207\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    204\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 207\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\dml\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:393\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[1;34m(self, world_size)\u001b[0m\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    389\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;241m*\u001b[39m i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_items) \u001b[38;5;241m/\u001b[39m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_items\n\u001b[0;32m    390\u001b[0m     )\n\u001b[0;32m    392\u001b[0m \u001b[38;5;66;03m# Backward\u001b[39;00m\n\u001b[1;32m--> 393\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;66;03m# Optimize - https://pytorch.org/docs/master/notes/amp_examples.html\u001b[39;00m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ni \u001b[38;5;241m-\u001b[39m last_opt_step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccumulate:\n",
      "File \u001b[1;32m~\\.conda\\envs\\dml\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\dml\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\dml\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train(data='data.yaml', epochs=50, batch=16, imgsz=640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cab987-63bb-4698-8208-c2e9330da63c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
