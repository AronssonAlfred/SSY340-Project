{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca8b413c-3993-4933-b899-b2edb0802f74",
   "metadata": {},
   "source": [
    "# Object Detection for Face Detection Project\n",
    "\n",
    "This notebook demonstrates the use of pre-trained object detection models, specifically focusing on detecting faces in images and live video feeds. The goal of this project is to implement and compare different object detection models, starting with YOLO (You Only Look Once), and assess their performance in terms of accuracy and speed.\n",
    "\n",
    "## Objectives:\n",
    "1. Research and select a suitable pre-trained object detection model (e.g., YOLOv5 or YOLOv8).\n",
    "2. Validate the model using a sample dataset to assess its performance.\n",
    "3. Implement the model to detect faces in static images and output annotated images with bounding boxes.\n",
    "4. Extend the functionality to handle live video feeds from a webcam, with real-time face detection and bounding box visualization.\n",
    "\n",
    "Throughout the notebook, we will evaluate key metrics such as accuracy (mAP) and inference speed (FPS), making adjustments as needed to optimize for both performance and accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d157ad50-97a1-4805-a86e-d9ca188dbc84",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e836d1da-44fa-4075-82d8-24bb1c7c9735",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd8ee4f-5c3a-4f23-bdf7-ac049ada3106",
   "metadata": {},
   "source": [
    "### Load Pre-Trained YOLO v8n model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd4751ef-2f3f-4a76-9062-079c77a8b5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('yolov8n.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfe0382-bd0a-4c2d-95c3-8d8983fb5752",
   "metadata": {},
   "source": [
    "### Verify Model Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39ef1ca0-e878-494d-9b4c-701ae396f42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\Alfred Aronsson\\SSY340-Project\\object-detection\\WIDER_train\\images\\1--Handshaking\\1_Handshaking_Handshaking_1_71.jpg: 480x640 5 persons, 1 tie, 118.2ms\n",
      "Speed: 4.0ms preprocess, 118.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "# Run inference on a sample image\n",
    "results = model('WIDER_train/images/1--Handshaking/1_Handshaking_Handshaking_1_71.jpg')\n",
    "\n",
    "# Display the results\n",
    "results[0].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf92ff2-72fe-4d45-8e2e-bfb4e648899c",
   "metadata": {},
   "source": [
    "### Convert Annotations From the Wider Format to the YOLO Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bceca317-9003-44c6-ba0e-462d1f3ffc79",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 60\u001b[0m\n\u001b[0;32m     57\u001b[0m         i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m num_boxes\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Convert the annotations to YOLO format\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m \u001b[43mconvert_to_yolo_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mannotations_file_training\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages_root_training\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_labels_dir_training\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[17], line 29\u001b[0m, in \u001b[0;36mconvert_to_yolo_format\u001b[1;34m(annotations_file, images_root, output_labels_dir)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Load image to get dimensions\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241m.\u001b[39mimread(image_full_path)\n\u001b[0;32m     30\u001b[0m image_height, image_width, _ \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Parse number of bounding boxes\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "\n",
    "# Training Data\n",
    "annotations_file_training = 'wider_face_split/wider_face_train_bbx_gt.txt'\n",
    "images_root_training = 'WIDER_train/images'\n",
    "output_labels_dir_training = 'output_labels'\n",
    "\n",
    "if not os.path.exists(output_labels_dir):\n",
    "    os.makedirs(output_labels_dir)\n",
    "\n",
    "# Function to parse annotations and convert them to YOLO format\n",
    "def convert_to_yolo_format(annotations_file, images_root, output_labels_dir):\n",
    "    with open(annotations_file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        # Parse image path\n",
    "        image_path = lines[i].strip()\n",
    "        image_full_path = os.path.join(images_root, image_path)\n",
    "        \n",
    "        # Skip if image does not exist\n",
    "        if not os.path.exists(image_full_path):\n",
    "            print(f\"Warning: Image {image_full_path} does not exist.\")\n",
    "            i += 1 + int(lines[i + 1].strip())\n",
    "            continue\n",
    "\n",
    "        # Load image to get dimensions\n",
    "        image = cv2.imread(image_full_path)\n",
    "        image_height, image_width, _ = image.shape\n",
    "\n",
    "        # Parse number of bounding boxes\n",
    "        num_boxes = int(lines[i + 1].strip())\n",
    "        label_path = os.path.join(output_labels_dir, image_path.replace('.jpg', '.txt'))\n",
    "        \n",
    "        # Ensure the output directory exists for the label file\n",
    "        label_dir = os.path.dirname(label_path)\n",
    "        if not os.path.exists(label_dir):\n",
    "            os.makedirs(label_dir)\n",
    "\n",
    "        with open(label_path, 'w') as label_file:\n",
    "            # Loop through bounding boxes\n",
    "            for j in range(num_boxes):\n",
    "                box_data = list(map(int, lines[i + 2 + j].split()[:4]))  # Extract x, y, width, height\n",
    "                x, y, width, height = box_data\n",
    "                \n",
    "                # Convert to YOLO format\n",
    "                x_center = (x + width / 2) / image_width\n",
    "                y_center = (y + height / 2) / image_height\n",
    "                width_normalized = width / image_width\n",
    "                height_normalized = height / image_height\n",
    "\n",
    "                # Write to label file in YOLO format\n",
    "                label_file.write(f\"0 {x_center} {y_center} {width_normalized} {height_normalized}\\n\")\n",
    "\n",
    "        # Move to the next image's annotations\n",
    "        i += 2 + num_boxes\n",
    "\n",
    "# Convert the annotations to YOLO format\n",
    "convert_to_yolo_format(annotations_file_training, images_root_training, output_labels_dir_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daec3ee-6b65-4e77-bb7e-a4658cbbff88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
